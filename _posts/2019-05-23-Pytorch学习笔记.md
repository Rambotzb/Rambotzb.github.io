# Pytorch学习笔记
## Pytorch简介与基础
pytorch就是神经网络中的numpy，可以将产生的tensor放在GPU中加速运算
那么什么是**Tensor**？tensor就是张量，是对矢量和矩阵向更高维度的泛化。下表给出了tensor的一些对应在数学上的实例。
| 张量的阶数 | 数学实例 |
|:-:|:-:|
| 0 | 标量(只有大小) |
| 1 | 矢量(有大小和方向)|
| 2 | 矩阵(数据表)|
| 3 | 3阶张量(数据立体)|
|……|……|
| n | 自行想象 |
### numpy与torch中tensor的互相转化
```python
np_data = np.arange(6).reshape((2, 3))
torch_data = torch.from_numpy(np_data)
torch2np = torch_data.numpy()
```
### torch中简单运算实例
```python
#abs
data = [-1, -2, 1, 2]
tensor = torch.FloatTensor(data)    #32bit
print(
    "\nnumpy", np.abs(data),
    "\ntensor", torch.abs(tensor)
)
```
### torch中矩阵运算实例
```python
#multiply
data = [[1, 2],[3, 4]]
tensor = torch.FloatTensor(data)    #32bit
print(
    "\nnumpy", np.matmul(data, data),
    "\ntensor", torch.mm(tensor, tensor),
)
```
**注意**：在新版本中dot点乘只能用于一维张量
***
### Variable变量
Variable的引入如下所示
```python
from torch.autograd import Variable
```
向variable中传递tensor，并且设置是否能进行BP。并尝试在X^2^的函数下，进行反向传播和查看grad。
```python
tensor = torch.FloatTensor([[1, 2], [3, 4]])
variable = Variable(tensor, requires_grad=True)
v_out = torch.mean(variable * variable)
v_out.backward()
print(variable.grad)
```
~~在新版本中variable和tensor被合并，requires_grad为默认true(后续查看文献后在进行补充)~~
### Activation Function(激励函数)
如下图所示为一些常见的激励函数
<img src="https://morvanzhou.github.io/static/results/torch/2-3-1.png">
现在使用torch中内置的激励函数来画一下函数图像
```python {.line-numbers}
import torch
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt

x = torch.linspace(-5, 5, 200)
x = Variable(x)
x_np = x.data.numpy()

y_relu = F.relu(x).data.numpy()
y_sigmoid = F.sigmoid(x).data.numpy()
y_tanh = F.tanh(x).data.numpy()
y_softplus = F.softplus(x).data.numpy()
#y_softmax = F.softmax(x)

plt.figure(1, figsize=(8, 6))
plt.subplot(221)
plt.plot(x_np, y_relu, c='red', label='relu')
plt.ylim((-1, 5))
plt.legend(loc='best')

plt.subplot(222)
plt.plot(x_np, y_sigmoid, c='red', label='sigmiod')
plt.ylim((-0.2, 1.2))
plt.legend(loc='best')

plt.subplot(223)
plt.plot(x_np, y_tanh, c='red', label='tanh')
plt.ylim((-1.2, 1.2))
plt.legend(loc='best')

plt.subplot(224)
plt.plot(x_np, y_softplus, c='red', label='softplus')
plt.ylim((-0.2, 6))
plt.legend(loc='best')

```
最后得到的图像和前面的一样
***
## 使用Pytorch搭建神经网络
### Regression(回归)
在本节课中，将使用pytorch来搭建第一个神经网络实现回归。具体代码如下：
```python {.line-numbers}
import torch
from torch.autograd import Variable
import torch.nn.functional as F
import matplotlib.pyplot as plt

x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) #x data (tensor), shape=(100,1)
y = x.pow(2) + 0.2*torch.rand(x.size())

x, y = Variable(x), Variable(y)  #在现在的版本中，variable与tensor合并了，这行可以注释

#plt.scatter(x.data.numpy(), y.data.numpy()) #这里是老版本的代码，在新版本中，可直接tenso.numpy()，并不需要取data
#plt.show()

class Net(torch.nn.Module):
	#必须的两个部分
	def __init__(self, n_features, n_hidden, n_output):
		super(Net, self).__init__()   #继承
		self.hidden = torch.nn.Linear(n_features, n_hidden)
		self.predict = torch.nn.Linear(n_hidden, n_output)
	
	def forward(self, x):
		x = F.relu(self.hidden(x))
		x = self.predict(x)
		return x

net = Net(1, 10, 1)
#print(net) 查看搭建好的神经网络

#实时打印出图像
plt.ion()
plt.show()

optimizer = torch.optim.SGD(net.parameters(), lr=0.5)
loss_func = torch.nn.MSELoss()

for i in range(101):
	prediction = net(x)
	
	loss = loss_func(prediction, y)  #注意前后位置，可能对结果有影响
	
	optimizer.zero_grad()   #将所有梯度设为零
	loss.backward()
	optimizer.step()
	if i%5 ==0:
		plt.cla()
		plt.scatter(x.data.numpy(), y.data.numpy())
		plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)
		plt.text(0.5, 0, 'Loss=%.4f' % loss.data, fontdict={'size':20, 'color': 'red' })
		plt.pause(0.1)
```
从生成的图像可以看出，随着训练代数的增加，图线逐渐向散点拟合。
***
### Classification(分类)
搭建一个能够实现分类功能的神经网络
```python

```
## 高级神经网络结构